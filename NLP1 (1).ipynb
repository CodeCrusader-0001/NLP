{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load streaming dataset\n",
        "dataset = load_dataset(\n",
        "    \"ai4bharat/IndicCorpV2\",\n",
        "    \"indiccorp_v2\",\n",
        "    split=\"hin_Deva\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# 2. Sentence tokenizer for Hindi\n",
        "def sentence_tokenizer(text):\n",
        "    sentence_endings = re.compile(r'(?<=[।!?\\.])\\s+')\n",
        "    return [s.strip() for s in sentence_endings.split(text.strip()) if s.strip()]\n",
        "\n",
        "# 3. Word tokenizer (handles Hindi, English, numbers, URLs, punctuation)\n",
        "def word_tokenizer(sentence):\n",
        "    token_pattern = re.compile(r\"\"\"\n",
        "        (https?://[^\\s]+) |                # URLs\n",
        "        ([\\w\\.-]+@[\\w\\.-]+) |               # Emails\n",
        "        (\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}) |   # Dates\n",
        "        (\\d+\\.\\d+) |                        # Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                # Hindi words\n",
        "        ([a-zA-Z]+) |                       # English words\n",
        "        (\\d+) |                             # Numbers\n",
        "        ([^\\s\\w\\u0900-\\u097F])              # Punctuation / symbols\n",
        "    \"\"\", re.VERBOSE | re.UNICODE)\n",
        "\n",
        "    return [m.group() for m in token_pattern.finditer(sentence)]\n",
        "\n",
        "# 4. Stats accumulators\n",
        "sentence_count = 0\n",
        "word_count = 0\n",
        "char_count = 0\n",
        "vocab_counter = Counter()\n",
        "MAX_DOCS = 1000\n",
        "\n",
        "# 5. Process and save tokenized sentences\n",
        "with open(\"tokenized_sentences.txt\", \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for i, example in tqdm(enumerate(dataset), total=MAX_DOCS):\n",
        "        if i >= MAX_DOCS:\n",
        "            break\n",
        "\n",
        "        text = example[\"text\"]\n",
        "        sentences = sentence_tokenizer(text)\n",
        "        sentence_count += len(sentences)\n",
        "\n",
        "        for s in sentences:\n",
        "            tokens = word_tokenizer(s)\n",
        "            word_count += len(tokens)\n",
        "            char_count += sum(len(tok) for tok in tokens)\n",
        "            vocab_counter.update(tokens)\n",
        "            f_out.write(\" \".join(tokens) + \"\\n\")\n",
        "\n",
        "# 6. Summary statistics\n",
        "unique_tokens = len(vocab_counter)\n",
        "ttr = unique_tokens / word_count if word_count > 0 else 0\n",
        "avg_sent_len = word_count / sentence_count if sentence_count > 0 else 0\n",
        "avg_word_len = char_count / word_count if word_count > 0 else 0\n",
        "\n",
        "print(\"===== Corpus Statistics =====\")\n",
        "print(f\"Total Documents Processed : {MAX_DOCS}\")\n",
        "print(f\"Total Sentences           : {sentence_count:,}\")\n",
        "print(f\"Total Words               : {word_count:,}\")\n",
        "print(f\"Total Characters          : {char_count:,}\")\n",
        "print(f\"Unique Tokens (Vocab Size): {unique_tokens:,}\")\n",
        "print(f\"Average Sentence Length   : {avg_sent_len:.2f} words\")\n",
        "print(f\"Average Word Length       : {avg_word_len:.2f} chars\")\n",
        "print(f\"Type/Token Ratio (TTR)    : {ttr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37VYc31Ghn3L",
        "outputId": "6f08c3e9-693e-4684-fbd5-be52425e6794"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 977.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Corpus Statistics =====\n",
            "Total Documents Processed : 1000\n",
            "Total Sentences           : 1,717\n",
            "Total Words               : 33,156\n",
            "Total Characters          : 126,918\n",
            "Unique Tokens (Vocab Size): 6,976\n",
            "Average Sentence Length   : 19.31 words\n",
            "Average Word Length       : 3.83 chars\n",
            "Type/Token Ratio (TTR)    : 0.2104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}