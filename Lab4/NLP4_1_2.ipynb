{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534d2e48",
   "metadata": {},
   "source": [
    "# N-gram Language Model Workflow\n",
    "\n",
    "This notebook demonstrates building n-gram language models from a tokenized Hindi corpus. The workflow includes:\n",
    "\n",
    "- **Loading Data:** The corpus is read from `sentence_tokenized.txt`.\n",
    "- **Counting N-grams:** Unigram, bigram, trigram, and quadrigram frequencies are computed using Python's `Counter`.\n",
    "- **Statistics:**  \n",
    "    - Total tokens (`N`): 22,801,825  \n",
    "    - Unique unigrams: 299,475  \n",
    "    - Unique bigrams: 3,466,685  \n",
    "    - Unique trigrams: 9,694,653  \n",
    "    - Unique quadrigrams: 14,174,147\n",
    "- **Saving Probabilities:** N-gram probabilities (raw, Add-One, Add-K, Token-Type) are calculated and saved to CSV files for further analysis.\n",
    "\n",
    "This process enables statistical modeling and probability estimation for natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30d10329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imp orts\n",
    "from collections import Counter\n",
    "import io, csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a372ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to tokenized Hindi file\n",
    "path = r\"C:\\Users\\ashis\\OneDrive\\Desktop\\NLP\\LAB1\\sentence_tokenized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5282b043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building n-gram counts: 1000000it [00:54, 18440.71it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique unigrams: 299475\n",
      "Unique bigrams: 3466685\n",
      "Unique trigrams: 9694653\n",
      "Unique quadrigrams: 14174147\n",
      "Total tokens (N): 22801825\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import io, csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def build_counts(path):\n",
    "    unigram_c = Counter()\n",
    "    bigram_c = Counter()\n",
    "    trigram_c = Counter()\n",
    "    quadrigram_c = Counter()\n",
    "\n",
    "    with io.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\", buffering=1024*1024) as f:\n",
    "        for line in tqdm(f, desc=\"Building n-gram counts\"):\n",
    "            words = line.strip().split()\n",
    "            if not words:\n",
    "                continue\n",
    "\n",
    "            # Add sentence boundaries\n",
    "            words = [\"<s>\",\"<s>\",\"<s>\"] + words + [\"</s>\"]\n",
    "\n",
    "            # Count n-grams\n",
    "            unigram_c.update(words)\n",
    "            bigram_c.update(zip(words[:-1], words[1:]))\n",
    "            trigram_c.update(zip(words[:-2], words[1:-1], words[2:]))\n",
    "            quadrigram_c.update(zip(words[:-3], words[1:-2], words[2:-1], words[3:]))\n",
    "\n",
    "    return unigram_c, bigram_c, trigram_c, quadrigram_c\n",
    "\n",
    "\n",
    "# Run builder\n",
    "unigram_c, bigram_c, trigram_c, quadrigram_c = build_counts(path=path)\n",
    "\n",
    "# Show stats\n",
    "print(\"Unique unigrams:\", len(unigram_c))\n",
    "print(\"Unique bigrams:\", len(bigram_c))\n",
    "print(\"Unique trigrams:\", len(trigram_c))\n",
    "print(\"Unique quadrigrams:\", len(quadrigram_c))\n",
    "\n",
    "# Totals\n",
    "N = sum(unigram_c.values())\n",
    "print(\"Total tokens (N):\", N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ef13f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size :  299475\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(unigram_c)\n",
    "print(\"Vocab Size : \" , vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "588485b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving unigram: 100%|██████████| 200000/200000 [00:01<00:00, 197216.92it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 200000 unigrams → unigram.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving bigram: 100%|██████████| 200000/200000 [00:01<00:00, 144674.43it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 200000 bigrams → bigram.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving trigram: 100%|██████████| 200000/200000 [00:01<00:00, 143648.54it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 200000 trigrams → trigram.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving quadrigram: 100%|██████████| 200000/200000 [00:01<00:00, 128759.18it/s]\n",
      "Saving quadrigram: 100%|██████████| 200000/200000 [00:01<00:00, 128759.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 200000 quadrigrams → quadrigram.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import io, csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_ngram_probs(counter, base_counter, filename_prefix, top_n=1000, k=0.6):\n",
    "    \"\"\"\n",
    "    Save n-gram probabilities with smoothing into CSV.\n",
    "    base_counter   : Denominator counts (None for unigrams)\n",
    "    top_n          : number of top n-grams to save\n",
    "    k              : value for Add-K smoothing\n",
    "    \"\"\"\n",
    "\n",
    "    V = vocab_size\n",
    "    csv_file = f\"{filename_prefix}.csv\"\n",
    "\n",
    "    # Precompute U values (unique continuations for each history)\n",
    "    U_dict = {}\n",
    "    if base_counter:  # only needed for bigram/trigram/quadrigram\n",
    "        for ng in counter:\n",
    "            hist = ng[:-1]\n",
    "            if hist not in U_dict:\n",
    "                U_dict[hist] = set()\n",
    "            U_dict[hist].add(ng[-1])\n",
    "        # convert sets to counts for faster lookup\n",
    "        U_dict = {hist: len(s) for hist, s in U_dict.items()}\n",
    "\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Ngram\", \"Count\", \"Add-One\", \"Add-K\", \"Token-Type\"])\n",
    "\n",
    "        for ngram, count in tqdm(counter.most_common(top_n), desc=f\"Saving {filename_prefix}\"):\n",
    "            key = \" \".join(ngram) if isinstance(ngram, tuple) else ngram\n",
    "\n",
    "            if base_counter:\n",
    "                denom = base_counter.get(ngram[:-1], 0)\n",
    "                U = U_dict.get(ngram[:-1], V)  # fallback if missing\n",
    "            else:  # unigram\n",
    "                denom = N\n",
    "                U = V\n",
    "\n",
    "            raw  = count\n",
    "            add1 = (counter[ngram] + 1) / (denom + V) \n",
    "            addk = (counter[ngram] + k) / (denom + k * V) \n",
    "            tokT = (counter[ngram] + k) / (denom + k * U) \n",
    "\n",
    "            writer.writerow([key, raw, add1, addk, tokT])\n",
    "\n",
    "    print(f\"Saved top {top_n} {filename_prefix}s → {csv_file}\")\n",
    "\n",
    "#Save all n-gram CSVs\n",
    "save_ngram_probs(unigram_c, None, \"unigram\", top_n=200000)\n",
    "save_ngram_probs(bigram_c, unigram_c, \"bigram\", top_n=200000)\n",
    "save_ngram_probs(trigram_c, bigram_c, \"trigram\", top_n=200000)\n",
    "save_ngram_probs(quadrigram_c, trigram_c, \"quadrigram\", top_n=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3abc7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved unigram → unigram.pkl (299475 n-grams)\n",
      "✅ Saved bigram → bigram.pkl (3466685 n-grams)\n",
      "✅ Saved bigram → bigram.pkl (3466685 n-grams)\n",
      "✅ Saved trigram → trigram.pkl (9694653 n-grams)\n",
      "✅ Saved trigram → trigram.pkl (9694653 n-grams)\n",
      "✅ Saved quadrigram → quadrigram.pkl (14174147 n-grams)\n",
      "✅ Saved quadrigram → quadrigram.pkl (14174147 n-grams)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Save n-gram counters to pickle format (2 columns: Ngram as tuple, Count)\n",
    "def save_ngrams_to_pickle(counter, filename_prefix):\n",
    "    \"\"\"\n",
    "    Save n-gram counter to pickle with 2 columns: Ngram (as tuple) and Count\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for ngram, count in counter.items():\n",
    "        data.append({\"Ngram\": ngram, \"Count\": count})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    pickle_file = f\"{filename_prefix}.pkl\"\n",
    "    \n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump(df, f)\n",
    "    \n",
    "    print(f\"✅ Saved {filename_prefix} → {pickle_file} ({len(df)} n-grams)\")\n",
    "\n",
    "# Save all n-gram counters to pickle\n",
    "save_ngrams_to_pickle(unigram_c, \"unigram\")\n",
    "save_ngrams_to_pickle(bigram_c, \"bigram\")\n",
    "save_ngrams_to_pickle(trigram_c, \"trigram\")\n",
    "save_ngrams_to_pickle(quadrigram_c, \"quadrigram\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
